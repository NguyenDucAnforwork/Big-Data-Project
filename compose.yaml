# Comments are provided throughout this file to help you get started.
# If you need more help, visit the Docker Compose reference guide at
# https://docs.docker.com/go/compose-spec-reference/

# Here the instructions define your application as a service called "app".
# This service is built from the Dockerfile in the current directory.
# You can add other services your application may depend on here, such as a
# database or a cache. For examples, see the Awesome Compose repository:
# https://github.com/docker/awesome-compose
services:
  broker:
    image: apache/kafka:latest
    hostname: broker
    container_name: broker
    user: root
    ports:
      - 9092:9092
    volumes:
      - kafka_data:/tmp/kraft-combined-logs
    environment:
      # Kraft setting
      KAFKA_BROKER_ID: "1"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: "1"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@broker:29093"

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: "0"
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
      # Bind internal listeners to 0.0.0.0 so services on the docker network can connect
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,PLAINTEXT_HOST://0.0.0.0:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      KAFKA_LOG_DIRS: "/tmp/kraft-combined-logs"
      CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"

  producer: 
    build: 
      context: .
      dockerfile: kafka/Dockerfile
    container_name: producer
    depends_on:
      - broker
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
    volumes:
      - ./data:/data
    # command: sh -c "sleep 10 && uv run python producer.py --speedup 3600.0"
    command: sh -c "sleep 20 && uv run python producer.py --speedup 120.0"
    restart: no

  # cassandra:
  #   image: cassandra:4.1
  #   container_name: cassandra
  #   ports:
  #     - "9042:9042"
  #   environment:
  #     - CASSANDRA_CLUSTER_NAME=TaxiCluster
  #     - CASSANDRA_DC=datacenter1
  #     - CASSANDRA_RACK=rack1
  #   volumes:
  #     - cassandra_data:/var/lib/cassandra
  #     - ./cassandra:/docker-entrypoint-initdb.d
  #   healthcheck:
  #     test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10

  # init-cassandra:
  #   image: cassandra:4.1
  #   container_name: init-cassandra-job
  #   depends_on:
  #     cassandra:
  #       condition: service_healthy
  #   volumes:
  #     - ./cassandra:/cassandra
  #   command: cqlsh cassandra -f /cassandra/init.cql

  # spark:
  #   build:
  #     context: .
  #     dockerfile: spark/Dockerfile
  #   container_name: spark
  #   depends_on:
  #     cassandra:
  #       condition: service_healthy  
  #     init-cassandra:
  #       condition: service_completed_successfully
  #     broker:
  #       condition: service_started
  #   ports:
  #     - "4040:4040"
  #   volumes:
  #     - ./spark:/opt/spark-apps
  #     - spark-checkpoints:/tmp/spark_checkpoints
  #   environment:
  #     - SPARK_LOCAL_DIRS=/tmp
  #     - KAFKA_BOOTSTRAP_SERVERS=broker:29092
  #     - CASSANDRA_HOST=cassandra
  #   command: >
  #     /opt/spark/bin/spark-submit
  #     --master local[*]
  #     --jars /opt/spark-jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,/opt/spark-jars/kafka-clients-3.4.1.jar,/opt/spark-jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,/opt/spark-jars/commons-pool2-2.11.1.jar,/opt/spark-jars/spark-cassandra-connector-assembly_2.12-3.5.1.jar
  #     --conf spark.sql.streaming.checkpointLocation=/tmp/spark_checkpoints
  #     /opt/spark-apps/streaming.py

  cassandra:
    image: cassandra:4.1
    container_name: cassandra
    ports:
      - "9042:9042"
    environment:
      - CASSANDRA_CLUSTER_NAME=TaxiCluster
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - MAX_HEAP_SIZE=512M
      - HEAP_NEWSIZE=128M
    volumes:
      - cassandra_data:/var/lib/cassandra
      - ./cassandra:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "cqlsh", "-e", "describe keyspaces"]
      interval: 10s
      timeout: 5s
      retries: 10

  init-cassandra:
    image: cassandra:4.1
    container_name: init-cassandra-job
    depends_on:
      cassandra:
        condition: service_healthy
    volumes:
      - ./cassandra:/cassandra
    command: cqlsh cassandra -f /cassandra/init_unified.cql

  spark:
    build:
      context: .
      dockerfile: spark/Dockerfile
    container_name: spark
    depends_on:
      cassandra:
        condition: service_healthy  
      init-cassandra:
        condition: service_completed_successfully
      broker:
        condition: service_started
    ports:
      - "4040:4040"
    volumes:
      - ./spark:/opt/spark-apps
      - spark-checkpoints:/tmp/spark_checkpoints
    environment:
      - SPARK_LOCAL_DIRS=/tmp
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
      - CASSANDRA_HOST=cassandra
    command: >
      /opt/spark/bin/spark-submit
      --master local[*]
      --driver-memory 1g
      --executor-memory 1g
      --jars /opt/spark-jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,/opt/spark-jars/kafka-clients-3.4.1.jar,/opt/spark-jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,/opt/spark-jars/commons-pool2-2.11.1.jar,/opt/spark-jars/spark-cassandra-connector-assembly_2.12-3.5.1.jar
      --conf spark.sql.streaming.checkpointLocation=/tmp/spark_checkpoints
      /opt/spark-apps/streaming_unified.py

  # spark-batch:
  #   build:
  #     context: .
  #     dockerfile: spark/Dockerfile
  #   container_name: spark-batch
  #   depends_on:
  #     cassandra:
  #       condition: service_healthy  
  #     init-cassandra:
  #       condition: service_completed_successfully
  #     broker:
  #       condition: service_started
  #   volumes:
  #     - ./spark:/opt/spark-apps
  #     - spark-checkpoints:/tmp/spark_checkpoints
  #   environment:
  #     - SPARK_LOCAL_DIRS=/tmp
  #     - KAFKA_BOOTSTRAP_SERVERS=broker:29092
  #     - CASSANDRA_HOST=cassandra
  #   command: uv run python3 /opt/spark-apps/scheduler.py --speedup 3600.0

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_SECURITY_ADMIN_USER=admin
      - GF_INSTALL_PLUGINS=hadesarchitect-cassandra-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - cassandra
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:29092
      - KAFKA_CLUSTERS_0_METRICS_PORT=9997
    depends_on:
      - broker
volumes:
  cassandra_data:
  grafana_data:
  spark-checkpoints:
  kafka_data:

    # The commented out section below is an example of how to define a PostgreSQL
    # database that your application can use. `depends_on` tells Docker Compose to
    # start the database before your application. The `db-data` volume persists the
    # database data between container restarts. The `db-password` secret is used
    # to set the database password. You must create `db/password.txt` and add
    # a password of your choosing to it before running `docker compose up`.
    #     depends_on:
    #       db:
    #         condition: service_healthy
    #   db:
    #     image: postgres
    #     restart: always
    #     user: postgres
    #     secrets:
    #       - db-password
    #     volumes:
    #       - db-data:/var/lib/postgresql/data
    #     environment:
    #       - POSTGRES_DB=example
    #       - POSTGRES_PASSWORD_FILE=/run/secrets/db-password
    #     expose:
    #       - 5432
    #     healthcheck:
    #       test: [ "CMD", "pg_isready" ]
    #       interval: 10s
    #       timeout: 5s
    #       retries: 5
    # volumes:
    #   db-data:
    # secrets:
    #   db-password:
    #     file: db/password.txt